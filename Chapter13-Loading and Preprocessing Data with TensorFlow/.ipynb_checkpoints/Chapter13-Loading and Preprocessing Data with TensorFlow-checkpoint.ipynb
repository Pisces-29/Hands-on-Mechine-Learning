{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde6a788",
   "metadata": {},
   "source": [
    "# 数据API\n",
    "让我们使用tf.data.Dataset.from_tensor_slices()在RAM中完全创建一个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbbcccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade12c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=tf.range(10)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0390ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2bdb0",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices()函数采用一个张量并创建一个tf.data.Dataset。其元素都是X的切片(沿着第一个维度)，因此数据集包含十个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed39876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7a839",
   "metadata": {},
   "source": [
    "## 链式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ba4e8",
   "metadata": {},
   "source": [
    "你可以通过如下的方式进行转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdc3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.repeat(3).batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3641df78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd80441",
   "metadata": {},
   "source": [
    "![Chaining dataset transformations](./chaining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc989a1f",
   "metadata": {},
   "source": [
    "数据集方法不会修改数据集，而是创建新的数据集，因此请保留对这些新数据集的引用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9743ae",
   "metadata": {},
   "source": [
    "你还可以用map()方法来调换元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed92b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.map(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fff4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a9585",
   "metadata": {},
   "source": [
    "通常会希望产生多个线程来加快处理速度，可以设置num_parallel_cells参数。注意传给map()方法的函数必须可以转换为TF函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c481ba",
   "metadata": {},
   "source": [
    "map()方法将转换应用于每个元素，但是apply()方法将转换应用于整个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5965d050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a4a3f62ff2d6>:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n"
     ]
    }
   ],
   "source": [
    "dataset=dataset.apply(tf.data.experimental.unbatch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cecaf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915120c4",
   "metadata": {},
   "source": [
    "可以使用filter()方法简单过滤数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbda489",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.filter(lambda x : x<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe2fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424a66b",
   "metadata": {},
   "source": [
    "查看数据集中的一些元素可以使用take()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292d17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a41b42",
   "metadata": {},
   "source": [
    "## 乱序数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71928a1",
   "metadata": {},
   "source": [
    "shuffle()方法进行乱序。它会创建一个新的数据集，该数据集首先将原数据集的第一项元素填充到缓冲区中。然后无论任何时候要求提供一个元素，它都会从缓冲区中随即取出一个元素，并用原数据集中的新元素替换它，直到遍历源数据为止，它将继续从缓冲区中随机抽取元素直到为空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66be777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=tf.data.Dataset.range(10).repeat(3)\n",
    "dataset=dataset.shuffle(buffer_size=5,seed=42).batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3202c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ddf98",
   "metadata": {},
   "source": [
    "### 交织来自多个文件的行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c952915",
   "metadata": {},
   "source": [
    "首先假设你已经加载了加州住房数据集，并对其进行乱序，然后将其分为训练集、验证集和测试集。之后将每个集合分成许多类似如下的csv文件(每行包含8个输入特征以及目标房屋中间值)："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cb661",
   "metadata": {},
   "source": [
    "我们还假设train_filepaths包含训练文件路径的列表(并且你还有valid_filepaths和test_filepaths)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4228e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116fffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c186575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c62b79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fccf88f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410ec06",
   "metadata": {},
   "source": [
    "现在让我们创建一个仅包含以下文件路径的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf2a417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset=tf.data.Dataset.list_files(test_filepaths,seed=42)\n",
    "# 匹配一个或多个 glob 模式的所有文件的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3ddee",
   "metadata": {},
   "source": [
    "默认情况下，list_files()函数返回一个乱序文件的文件路径的数据集。通常这是一件好事，但是如果出于某种原因不希望这样做，则可以设置shuffle=False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf50324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'datasets\\\\housing\\\\my_test_00.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in filepath_dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a717b",
   "metadata": {},
   "source": [
    "接下来，你可以调用interleave()方法一次读取5个文件并交织在它们的行(使用skip()方法跳过每个文件的第一个行,即行标题)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f52eaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers=5\n",
    "dataset=filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filenames=filepath).skip(1),cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2314ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InterleaveDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294cdcc",
   "metadata": {},
   "source": [
    "interleave()方法将创建一个数据集，该数据集将从filepath_dataset中拉出5个文件路径，对于每个路径，它将调用你为其提供的函数来创建新的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df65133",
   "metadata": {},
   "source": [
    "此阶段共有7个数据集：文件路径数据集、交织数据集和由交织数据集在内部创建的5个TextLineDatasets，当我们遍历交织数据集时，它将循环遍历这5个TextLineDatasets，每次读取一行，直到所有的数据集都读出为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b32a5",
   "metadata": {},
   "source": [
    "默认情况下。interleave()不使用并行。它只是顺序地从每个文件中一次读取一行。如果你希望它并行的读取文件，则可以将num_parallel_calls设置为所需要的线程数.也可以设置成tf.data.experimental.AUTOTUNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d3ac819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'5.3623,15.0,7.55956678700361,1.1407942238267148,937.0,3.3826714801444044,33.73,-116.89,2.013'\n",
      "b'5.5968,23.0,5.783870967741936,1.0290322580645161,1145.0,3.693548387096774,37.4,-121.85,2.432'\n",
      "b'1.2012,12.0,1.4657534246575343,0.8986301369863013,1194.0,3.271232876712329,34.05,-118.27,2.75'\n",
      "b'6.2427,19.0,6.446293494704992,1.0257186081694403,2621.0,3.9652042360060515,33.85,-118.08,2.887'\n",
      "b'3.2596,33.0,5.017656500802568,1.0064205457463884,2300.0,3.691813804173355,32.71,-117.03,1.03'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62e7dd",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e805e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个执行预处理的小函数\n",
    "n_inputs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2849f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    defs=[0.]*n_inputs+[tf.constant([],dtype=tf.float32)]\n",
    "    fields=tf.io.decode_csv(line,record_defaults=defs)\n",
    "    x=tf.stack(fields[:-1])\n",
    "    y=tf.stack(fields[-1:])\n",
    "    return (x-X_mean)/X_std,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e771a7c",
   "metadata": {},
   "source": [
    "该函数的解释看书，并不复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbaa0e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.7702099 , -1.0778131 ,  0.82483   ,  0.08842757, -0.44469705,\n",
       "         0.1794734 , -0.89780813,  1.3465551 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.013], dtype=float32)>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'5.3623,15.0,7.55956678700361,1.1407942238267148,937.0,3.3826714801444044,33.73,-116.89,2.013')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b19b07",
   "metadata": {},
   "source": [
    "## 合并在一起\n",
    "我们构造一个小的辅助函数：它将创建并返回一个数据集，该数据集有效地从多个csv文件中加载加州住房数据，对其进行预处理、随机乱序，可以选择重复并进行批处理。\n",
    "![preprocessing](./preprocess.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "568a86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_reader_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(\n",
    "        filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_reader_threads)\n",
    "    dataset=dataset.map(preprocess,num_parallel_calls=n_parse_threads)\n",
    "    dataset=dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0588713",
   "metadata": {},
   "source": [
    "## 预取\n",
    "通过最后调用prefetch(1)，我们正在创建一个数据集，该数据集将尽最大可能总是提前准备一个批次。换句话说，当我们训练算法正处理一个批次的时候，数据集已并行工作准备下一个批次了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6072d",
   "metadata": {},
   "source": [
    "## 和tf.keras一起使用数据集\n",
    "现在我们可以使用csv_reader_datasets()函数为训练集创建数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22a3875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=csv_reader_dataset(train_filepaths)\n",
    "valid_set=csv_reader_dataset(valid_filepaths)\n",
    "test_set=csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "658135de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 0.6259667 , -0.44494775,  0.3778275 ,  0.06992896,  0.10104288,\n",
      "        -0.00773004, -0.76194876,  0.8917654 ],\n",
      "       [ 4.0929923 , -0.91959685,  1.5314441 ,  0.06020879,  0.27170068,\n",
      "         3.918851  ,  1.0276414 , -1.2472363 ],\n",
      "       [ 0.3574874 , -0.04940685, -0.22534129, -0.10014284,  0.6997141 ,\n",
      "        -0.26909292, -0.80411196,  0.5819124 ],\n",
      "       [ 0.50157374, -1.868895  ,  0.46941856, -0.03959163,  1.093961  ,\n",
      "         0.03559649, -0.01706686,  0.96173245],\n",
      "       [-0.94478816,  0.74167496, -0.61156577, -0.10433538,  0.7617715 ,\n",
      "         0.3229953 , -0.75726473,  0.6118972 ],\n",
      "       [-1.2130054 , -1.3942459 , -0.2613689 ,  0.03043593,  1.215338  ,\n",
      "        -0.3110435 , -0.57924217,  1.1466476 ],\n",
      "       [-0.27793753,  1.3745404 ,  0.09625438, -0.13260026, -0.71847963,\n",
      "         0.02544731, -0.7760026 ,  0.7618288 ],\n",
      "       [-1.0562965 , -0.20762321, -0.14830488,  0.12707646, -0.6618979 ,\n",
      "        -0.2529792 ,  1.9318062 , -1.0073464 ],\n",
      "       [ 0.32511905,  1.6909732 ,  0.11278843, -0.26914397, -0.6180927 ,\n",
      "         0.09885789, -0.88843834,  0.851787  ],\n",
      "       [-0.11012501, -0.91959685, -0.2705761 , -0.12003419, -0.31693184,\n",
      "        -0.2550172 , -0.85564494,  0.7768212 ],\n",
      "       [-1.1567012 , -1.0778131 , -0.04081136, -0.41298354, -1.1255031 ,\n",
      "         0.24484429,  1.5663931 , -1.6220614 ],\n",
      "       [-0.8066203 , -1.2360295 , -0.23060684, -0.10438634,  0.30090415,\n",
      "        -0.2807913 ,  2.1238825 , -1.3172033 ],\n",
      "       [ 0.11341568,  0.5043504 ,  0.20260498,  0.47410664, -0.6719366 ,\n",
      "        -0.11491862, -0.64014494,  1.0267047 ],\n",
      "       [-0.4374747 , -1.1569214 ,  0.09415399, -0.06247256,  2.3068178 ,\n",
      "        -0.11761209,  1.4164782 , -0.6125301 ],\n",
      "       [-0.31648627,  1.5327568 , -0.04889712, -0.06495529, -0.42188185,\n",
      "         0.03211418,  0.9714244 , -1.4371462 ],\n",
      "       [-0.31920978, -0.91959685, -0.3891536 ,  0.25152627,  0.9452058 ,\n",
      "         0.10698049, -0.8509591 ,  0.8018072 ],\n",
      "       [-0.7835225 ,  0.82078314, -0.9457335 , -0.26127434, -1.1337166 ,\n",
      "        -0.40606186, -0.70573175,  0.6768655 ],\n",
      "       [-0.8253709 , -1.2360295 , -0.5209145 , -0.12347274,  1.5237997 ,\n",
      "        -0.05206117, -1.164841  ,  1.1916286 ],\n",
      "       [ 2.9036396 , -1.6315705 ,  0.4324402 , -0.4036223 , -1.128241  ,\n",
      "        -0.08222339, -0.9821345 ,  1.0766805 ],\n",
      "       [-0.46460542, -1.5524622 , -0.567691  , -0.12695865,  0.00521898,\n",
      "        -0.43467546,  0.8074556 , -1.2222463 ],\n",
      "       [-0.59800696, -0.7613805 , -0.6745284 , -0.53508353, -0.9767479 ,\n",
      "         0.47010586, -0.80411196,  0.6718706 ],\n",
      "       [-0.37849933,  1.0581076 , -0.75512016, -0.09470024,  0.24067199,\n",
      "         0.40168914, -0.72447133,  0.6718706 ],\n",
      "       [ 0.00714479, -1.7106786 ,  0.07815559, -0.10522255,  3.1655824 ,\n",
      "         0.11019504, -0.16229604,  0.28704795],\n",
      "       [-0.10179716,  0.5834586 , -0.03945715, -0.14152162,  0.43688285,\n",
      "         0.04261018, -1.3662872 ,  1.2515981 ],\n",
      "       [ 0.6062209 ,  0.5043504 ,  0.46918726, -0.14729793, -0.393591  ,\n",
      "        -0.06512372,  1.3883705 , -0.8874034 ],\n",
      "       [ 0.89795506, -0.20762321,  0.22016044, -0.04087295,  0.26622504,\n",
      "         0.3301863 ,  0.8496188 , -1.1622767 ],\n",
      "       [ 0.379695  , -1.3942459 , -0.29842633, -0.39056924,  0.07183941,\n",
      "         0.24108876, -0.72915536,  0.98672235],\n",
      "       [ 1.4902745 , -1.1569214 ,  0.86841744, -0.1249761 ,  1.1861345 ,\n",
      "         0.15375943,  1.0791744 , -1.1972603 ],\n",
      "       [-0.77257603,  0.1879177 , -0.5295967 , -0.17056635, -0.32879576,\n",
      "         0.5712023 , -0.75726473,  0.7068541 ],\n",
      "       [ 0.827614  ,  0.74167496,  0.17022714, -0.30621356, -0.17912796,\n",
      "         0.14117102,  0.8777282 , -1.347188  ],\n",
      "       [-0.57255226, -1.1569214 , -0.34077233,  0.03731148,  0.35566068,\n",
      "         0.15135384, -0.88843834,  0.8118047 ],\n",
      "       [ 0.68598944,  0.6625668 ,  0.28975236, -0.15033233, -0.75224614,\n",
      "        -0.17637718, -0.79474217,  0.7218465 ]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[2.306  ],\n",
      "       [5.00001],\n",
      "       [3.822  ],\n",
      "       [0.872  ],\n",
      "       [1.207  ],\n",
      "       [1.083  ],\n",
      "       [1.744  ],\n",
      "       [0.84   ],\n",
      "       [1.954  ],\n",
      "       [2.453  ],\n",
      "       [1.139  ],\n",
      "       [0.889  ],\n",
      "       [1.08   ],\n",
      "       [1.188  ],\n",
      "       [2.215  ],\n",
      "       [2.285  ],\n",
      "       [1.75   ],\n",
      "       [0.916  ],\n",
      "       [4.656  ],\n",
      "       [2.56   ],\n",
      "       [0.906  ],\n",
      "       [1.413  ],\n",
      "       [0.809  ],\n",
      "       [1.22   ],\n",
      "       [1.441  ],\n",
      "       [2.268  ],\n",
      "       [1.44   ],\n",
      "       [2.674  ],\n",
      "       [1.255  ],\n",
      "       [3.979  ],\n",
      "       [1.761  ],\n",
      "       [3.02   ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_set.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f741780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d992e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9505dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 15s 2ms/step - loss: 2.4037 - val_loss: 1.1860\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.7466 - val_loss: 0.6537\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.6338 - val_loss: 0.5988\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.6019 - val_loss: 0.5645\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 893us/step - loss: 0.5723 - val_loss: 0.5352\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 873us/step - loss: 0.5531 - val_loss: 0.5136\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.5264 - val_loss: 0.5031\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.5213 - val_loss: 0.4799\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.4833 - val_loss: 0.4763\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.4884 - val_loss: 0.4571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x205098e4688>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, epochs=10,validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061f4e8",
   "metadata": {},
   "source": [
    "将数据集和验证集直接传递给fit()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7a7e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 595us/step - loss: 0.4771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47709891200065613"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c883ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2170458 ],\n",
       "       [2.5704403 ],\n",
       "       [5.0930634 ],\n",
       "       [1.7192392 ],\n",
       "       [2.387019  ],\n",
       "       [1.7833905 ],\n",
       "       [1.4368004 ],\n",
       "       [1.6693234 ],\n",
       "       [1.5936759 ],\n",
       "       [1.6352576 ],\n",
       "       [2.0822856 ],\n",
       "       [1.0413637 ],\n",
       "       [2.2652988 ],\n",
       "       [2.8106225 ],\n",
       "       [1.2317045 ],\n",
       "       [2.369283  ],\n",
       "       [1.772979  ],\n",
       "       [2.0483487 ],\n",
       "       [3.2839038 ],\n",
       "       [2.586651  ],\n",
       "       [2.248283  ],\n",
       "       [3.6538444 ],\n",
       "       [1.6296895 ],\n",
       "       [2.3292778 ],\n",
       "       [1.5256877 ],\n",
       "       [2.2392294 ],\n",
       "       [3.516539  ],\n",
       "       [0.8947282 ],\n",
       "       [2.1898682 ],\n",
       "       [2.265708  ],\n",
       "       [1.7842765 ],\n",
       "       [1.5890962 ],\n",
       "       [2.178831  ],\n",
       "       [1.8324475 ],\n",
       "       [2.8765388 ],\n",
       "       [4.368442  ],\n",
       "       [1.967953  ],\n",
       "       [1.9726961 ],\n",
       "       [2.5781004 ],\n",
       "       [1.5926174 ],\n",
       "       [1.5536399 ],\n",
       "       [3.1279664 ],\n",
       "       [1.0283985 ],\n",
       "       [3.1463778 ],\n",
       "       [1.0768527 ],\n",
       "       [3.8612533 ],\n",
       "       [1.6391737 ],\n",
       "       [1.4020269 ],\n",
       "       [1.6194077 ],\n",
       "       [1.7219038 ],\n",
       "       [2.976847  ],\n",
       "       [2.7150078 ],\n",
       "       [1.9398501 ],\n",
       "       [0.9723617 ],\n",
       "       [1.1952841 ],\n",
       "       [3.3255959 ],\n",
       "       [1.7710216 ],\n",
       "       [0.55482095],\n",
       "       [2.1455703 ],\n",
       "       [1.1496989 ],\n",
       "       [1.612881  ],\n",
       "       [1.5276809 ],\n",
       "       [1.0759139 ],\n",
       "       [1.4585435 ],\n",
       "       [1.905606  ],\n",
       "       [1.479755  ],\n",
       "       [3.774881  ],\n",
       "       [1.8845677 ],\n",
       "       [1.8451581 ],\n",
       "       [1.5493886 ],\n",
       "       [1.0166125 ],\n",
       "       [1.9047372 ],\n",
       "       [3.2898068 ],\n",
       "       [0.30675763],\n",
       "       [1.6994967 ],\n",
       "       [1.9786963 ],\n",
       "       [2.140738  ],\n",
       "       [1.8524313 ],\n",
       "       [2.0587907 ],\n",
       "       [3.944386  ],\n",
       "       [0.9032096 ],\n",
       "       [2.8061135 ],\n",
       "       [1.9319632 ],\n",
       "       [1.6699967 ],\n",
       "       [2.15412   ],\n",
       "       [1.0864831 ],\n",
       "       [1.9512632 ],\n",
       "       [2.9842465 ],\n",
       "       [1.778384  ],\n",
       "       [2.1513677 ],\n",
       "       [2.9409738 ],\n",
       "       [1.0940067 ],\n",
       "       [1.4153825 ],\n",
       "       [3.372946  ],\n",
       "       [2.5260565 ],\n",
       "       [1.9948602 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set=test_set.take(3).map(lambda X,y:X)\n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73adc89",
   "metadata": {},
   "source": [
    "到目前为止，我们使用了csv文件，这些文件是通用的，简单且便捷的，但是效率并不高，并不能很好的支持大型或复杂的数据结构。因此让我们使用TFrecords。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef92cc",
   "metadata": {},
   "source": [
    "# TFRecord格式\n",
    "你可以使用tf.io.TFRecordWriter类轻松的建立TFRecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9329d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ace13e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths=[\"my_data.tfrecord\"]\n",
    "dataset=tf.data.TFRecordDataset(filenames=filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc42b9",
   "metadata": {},
   "source": [
    "默认情况下，TFRecordDataset将一个接一个地读取文件，但是你可以通过设置num_parallel_reads使其并行读取多个文件并交织记录。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576d60f",
   "metadata": {},
   "source": [
    "## 压缩的TFRecord文件\n",
    "你可以通过设置options参数来创建压缩的TFRecord文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "146ae504",
   "metadata": {},
   "outputs": [],
   "source": [
    "options=tf.io.TFRecordOptions(compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfbdd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\",options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1694c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=tf.data.TFRecordDataset(filenames='my_compressed.tfrecord',compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "756fb8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7bb46c",
   "metadata": {},
   "source": [
    "## 协议缓冲区简介\n",
    "不重要，看书。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c459e",
   "metadata": {},
   "source": [
    "## TensorFlow协议"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd5f9e",
   "metadata": {},
   "source": [
    "## 加载和解析Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7729fe5",
   "metadata": {},
   "source": [
    "## 使用SequenceExample Protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b5792",
   "metadata": {},
   "source": [
    "# 预处理输入特征\n",
    "让我们看看模型中的预处理层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37fe4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例如这是使用Lambda层实现标准化的方法。\n",
    "means=np.mean(X_train,axis=0,keepdims=True)\n",
    "stds=np.std(X_train,axis=0,keepdims=True)\n",
    "eps=keras.backend.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8fb4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([keras.layers.Lambda(lambda inputs:(inputs-means)/(stds+eps)),keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d17f39",
   "metadata": {},
   "source": [
    "这并不难，但是你希望使用一个很好的自包含自定义层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef32d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def adapt(self,data_sample):\n",
    "        self.means_=np.mean(data_sample,axis=0,keepdims=True)\n",
    "        self.stds_=np.std(data_sample,axis=0,keepdims=True)\n",
    "    def call(self,inputs):\n",
    "        return (inputs-self.means_)/(self.stds_+keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f4dd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer=Standardization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b61a9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49796ffd",
   "metadata": {},
   "source": [
    "在使用该层之前，你需要调用adapt方法()来使其适应数据集并将其传递给数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e322d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(std_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54fdde",
   "metadata": {},
   "source": [
    "keras.layers.Normal标准化层就是使用的该原理，现在可以直接使用。使用方法跟上面相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e74b4d",
   "metadata": {},
   "source": [
    "## 使用one-hot编码分类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9e04ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing=pd.read_csv('./housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf43023a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31a56b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[\"<1H OCEAN\",\"INLAND\",\"NEAR OCEAN\",\"NEAR BAY\",\"ISLAND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ed48298",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=tf.range(len(vocab),dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84972c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_init=tf.lookup.KeyValueTensorInitializer(vocab,indices)\n",
    "num_oov_buckets=2\n",
    "table=tf.lookup.StaticVocabularyTable(table_init,num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3e3cd",
   "metadata": {},
   "source": [
    "- 首先我们定义词汇表，这是所有可能类别的列表。\n",
    "- 然后我们创建带有相应索引的张量。\n",
    "- 接下来我们为查找表创建一个初始化程序，将类别列表及其对应的索引传递给它。在此示例中，我们已有此数据，因此我们使用KeyValueTensorInitializer。\n",
    "- 在最后两行中，我们创建了查找表，为其提供了初始化程序并指定了词汇表外桶的数量。如果我们查找词汇表中不存在的类别，则查找表将计算该类别的哈希并将在这个未知类别分配给ovv桶中的一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c1ac2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们使用查找表将一小部分数据转换成one-hot编码：\n",
    "categories=tf.constant([\"NEAR BAY\",\"DESERT\",\"INLAND\",\"INLAND\"])\n",
    "cat_indices=table.lookup(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "889070bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83f53e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_one_hot=tf.one_hot(cat_indices,depth=len(vocab)+num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f70ecc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91175e1",
   "metadata": {},
   "source": [
    "## 使用Embedding编码分类特征\n",
    "嵌入是表示类别的可训练密集向量。默认情况下，嵌入是随机初始化的，例如，\"NEAR BAY\"类别最初可以由诸如[0.131,0.890]的随机向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795f94f",
   "metadata": {},
   "source": [
    "让我们看看如何手动实现embedding以了解它的工作原理，首先我们要创建一个包含每个类别embedding的Eembedding矩阵，并随机初始化。每个类别和每个ovv桶都有一行，每个嵌入维度都有一列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "302cdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=2\n",
    "embed_init=tf.random.uniform([len(vocab)+num_oov_buckets,embedding_dim])\n",
    "embedding_matrix=tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae95c38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.7413678 , 0.62854624],\n",
       "       [0.01738465, 0.3431449 ],\n",
       "       [0.51063764, 0.3777541 ],\n",
       "       [0.07321596, 0.02137029],\n",
       "       [0.2871771 , 0.4710616 ],\n",
       "       [0.6936141 , 0.07321334],\n",
       "       [0.93251204, 0.20843053]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b979107",
   "metadata": {},
   "source": [
    "让我们对之前相同的分类特征进行编码，但这次使用Embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b84ccacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=tf.constant([\"NEAR BAY\",\"DESERT\",\"INLAND\",\"INLAND\"])\n",
    "cat_indices=table.lookup(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4031cee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a75a6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.07321596, 0.02137029],\n",
       "       [0.6936141 , 0.07321334],\n",
       "       [0.01738465, 0.3431449 ],\n",
       "       [0.01738465, 0.3431449 ]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix,cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de8d78a",
   "metadata": {},
   "source": [
    "tf.nn.embedding_lookup()函数以给定的索引查找在嵌入矩阵中的行，这就是它所做的全部。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce8a9d",
   "metadata": {},
   "source": [
    "keras提供了一个keras.layers.Embedding层来处理Embedding矩阵。创建层时，它将随机初始化嵌入矩阵，然后使用某些类别索引调用的时，它将返回嵌入矩阵这些索引处的行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87c080c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets,output_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec6fa1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[ 0.01289039,  0.0191061 ],\n",
       "       [ 0.01639661, -0.01945841],\n",
       "       [ 0.00692506, -0.00518861],\n",
       "       [ 0.00692506, -0.00518861]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab033f",
   "metadata": {},
   "source": [
    "把所有内容放在一起，我们现在可以创建一个keras模型，该模型可以处理分类特征并学习每个类别的Embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0945f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs=keras.layers.Input(shape=[8])\n",
    "categories=keras.layers.Input(shape=[],dtype=tf.string)\n",
    "cat_indices=keras.layers.Lambda(lambda cats:table.lookup(cats))(categories)\n",
    "cat_embed=keras.layers.Embedding(input_dim=6,output_dim=2)(cat_indices)\n",
    "encoded_inputs=keras.layers.concatenate([regular_inputs,cat_embed])\n",
    "outputs=keras.layers.Dense(1)(encoded_inputs)\n",
    "model=keras.models.Model(inputs=[regular_inputs,categories],outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9bd70",
   "metadata": {},
   "source": [
    "## keras预处理层\n",
    "- keras.layers.Normalization层，执行特征标准化。\n",
    "- TextVectorization层，能够将输入中的每个单词编码为它在词汇表中的索引。\n",
    "在这两种情况下你创建层，并使用数据样本调用adapt()方法，然后再模型中正常使用该层。\n",
    "- keras.layers.Discretization层，它将数据切成不同的离散块，并将每个块编码成一个one-hot。Discretization类不可微分，应该仅在模型开始时使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4093e7",
   "metadata": {},
   "source": [
    "你也可以用preprocessingStage类连接多个预处理层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c4dc2",
   "metadata": {},
   "source": [
    "TextVectorization层还可以选择输出单词计数向量，而不是单词索引。应该减少常用单词重要性的方式对单词计数进行归一化，此技术称为TF-IDF。详见书，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12f17e",
   "metadata": {},
   "source": [
    "# TF Transform\n",
    "详见书。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830080a4",
   "metadata": {},
   "source": [
    "# TensorFlow数据项目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018f302",
   "metadata": {},
   "source": [
    "调用tfds.load()函数，它会下载你想要的数据集，并将该数据作为数据集的目录返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cbe2e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\10093\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02480a2af53f4c939858ccddd55ba4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d661f58a364820807db799ad960728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab34240748842179dbfba0e6d1fd278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-train.tfrecord...:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-test.tfrecord...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\10093\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "dataset=tfds.load(name='mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b6e355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train,mnist_test=dataset[\"train\"],dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad455a3",
   "metadata": {},
   "source": [
    "然后你可以应用你所需要的任何转换，并且准备好训练你的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1755ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train=mnist_train.shuffle(10000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b9bb120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAABmCAYAAAA9I6EzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADz5JREFUeJzt3XmwnNOfx/H38fuF2BISTCpiGbGEwijGEiSWIAplrPUzmNjGWMrO2EIh1t/EvoxQlrIVsW9Bxh5mFCEkIsSWhEIkSBBiSZz54+Zzn+6n75V7c/v206fzeVWlbnpJ9+knT5/7fc75nu8JMUbMzCxNSxTdADMzW3TuxM3MEuZO3MwsYe7EzcwS5k7czCxh7sTNzBLWcJ14COHfQgifhxA+DCHsUHR76kEIoV8IYXwIYUYI4fSi21MPfJ6UC03mhRC+KfnTreh21ZMQwpgQwmVFtyPvr0U3oJpCCP8A/BfwT0Av4OkQQp/oZPibgf8ExgOTQwgjY4zTCm5TYXyetGh5YFqMsW/RDalHIYTBwADg/4puS15DdeJAP2BqjHEGMCOEsAywMjCj2GYVJ4SwGrBijPF/FtweDPxQbKsK5/OkUjdgVtGNqGPDgOeKbkRLGm04ZTLQJ4TQO4SwOTAb+LbgNhVtfWBmCOGFEMIMYGCMcXH/svo8qdQNWKFk2O0/im5QvQgh/AvwHfC/RbelJQ0ViccYp4cQ/huYRtMvqCExxvkFN6toKwH9gS1p6qzeDyE8GGOcUmyziuPzpEVLAFOAg4A+wCshhPtjjLOLbVaxQgiBpij834HdC25OixoqEg8hbAvsD6wCrA4MCyGsVWyrCjcfmBBjnBBj/AyYCGxacJsK5fOkUoxxYoxx5xjjjBjjOOBHYJ2i21UH/gZMiTGOLbohrWmoSBzYGhizYLhgVghhIvDPwKfFNqtQU4HlSm7/Ffi9mKbUDZ8nOSGEHYHuMcZHFtzVlaaOfHG3PTAghDCdBd+jEMKsGOPfC21ViYaKxIFJwKAQQvcQQm+ahhAmFdymoo0DeoQQ+ocQ/hFYD3iz4DYVzedJpa7AuSGEpUMIu9M0yflxwW0qXIzx6BhjzxhjL+By4Pp66sChwSLxGOOTIYRBNE1c/QFcEWOcWHCzChVj/D2EMAQYCfwFOCPG+GXBzSqUz5NKMcanQgi70HTlNhP41xjjvGJbZW0RFu/UWDOztDXacIqZ2WLFnbiZWcLciZuZJcyduJlZwtyJm5klrBYphotL+ktox3N9TFrm41LJx6SSj0kJR+JmZglzJ25mljB34mZmCXMnbmaWMHfiZmYJcyduZpYwd+JmZglrqFK0Vun9998H4OeffwZg/fXXB2CZZZYprE0pePHFFwEYPnw4AL/++isAo0aNAqBr167FNKyGpk+fDsCZZ54JwFdffQXA6NGjC2tTe/z4Y9OeFrvv3rSr2tlnn9382K677lpImzqDI3Ezs4Q5Em9QDz/8MACHHHIIAD/99BMAQ4cOBeDCCy8spmF16ttvmza7VwR+0EEHAfDbb7+VPe/pp58GYO+9965h66rr99+bdue7/PLLAZg7dy4Ac+bMAeCbb74B4M03mzaA+uCDDwC46qqratrOjtL/3dixTdtjvvvuu82PORI3M7O6kEQkPn78+LLbijIVTWp3ohCaSg2sssoqQBZ1nnDCCWWPy3777QfAtttuC2RRK0D37t2r9wFqaObMmQDsu+++QPaZNQa+9NJLF9OwOqfI+pVXXgGgV69eZfffeOONQHZFkzJddej70Rp9r3bccUcADj744M5tWJX17NkTyKLuxx57rPmxI444AoAePXrUvmFV5kjczCxhtdhjc5HeoHTM9oYbbih7TNFm8xvkIvGKBrTx8Y8++qj5vr59+7azxcVWYdMx2W233QB46623ANhggw0AePDBBwHo169ftd/6z9R9FcNnn30WgD322APIrlTuv/9+AO677z4Abr/9dgCmTJkCwJprrtmRty30XFHWSe/evct+nnHGGQBsvvnmQHb1ttpqqwGw4oorVrsppTrtmOj/8oADDmi+T3Med911V3teqoLmDfT9uuyyyzr0ejmuYmhm1ujciZuZJazuJjY1LKDJy9L7ZMkllwSyS9o//vgDgHPPPReAlVdeuez5Gi7Rpc95551X9vgKK6wAQJcuXTrc/qJcc801QDaMos+83nrrATUfRql7OmdOP/10IEtHe/755wHYcsstATjmmGOAbNJ7pZVWqmk7O8Oyyy4LZENtffr0AeD4448vrE2daamllqq475577gE6Ppyi4UulZVZ5OKVNHImbmSWs7iJxRdHbbbdd830TJkwoe86pp54KwEUXXdSu125tEvfRRx8FYPXVV2/X69WTSy65BMgmbxV5dzTSaFSTJ08G4IsvvgBgk002AWDrrbcGYMyYMQB8+umnAFx//fUALLfccjVtZ2fQhKVKMPzwww9FNqdmOiOJQ69ZgwSRVjkSNzNLWN1F4m2h8cy20mKhJ554AsjGwE8++WSg2N+iHaW5g/xnOPDAAwEXumqNUsKWWKIpjlGqoXz++ecAbLTRRgDstNNONWxd59Ic00MPPQTALrvsUmRzOt2gQYOA8hTJ2bNnV+W1deWrn7/88gtQ2wJpjsTNzBJWt4t9NNsL2bLf9957D4BVV10VyJZIr7HGGn/6Wt9//33Zaz733HMAnHXWWU0NXHAMSpf3L8L4eCELOLQwQ1kpigjmz59frbfoiLpd7HP00UcD8MILLwDw4YcfAjBx4kQgi8BHjBgBwFFHHVXNty90sc/rr78OQP/+/YEsEn/mmWdafL4KRymDZ7PNNqt2k6AGx0TlOCDrC9p7VZ+nY/H2228DcMsttwBw+OGHd+h1F/BiHzOzRle3Y+Kl+binnXYakEVDyijQUmlFU/n8cFExq1mzZgHZUmpF6HvuuWfFe9Y7bfagMqG6mlCp1I6aNm0aUH5FlNdJEVlNKOMpnxutzR+UhaKxcx1XLeHWWoUUKU9ctNlDPpNJpWo1P6CrO22yoCyxVM4DLbWHbF1FR6nQnCJx9Sm15EjczCxhdRuJlxoyZAgAF198MQAff/wxAJMmTQKyMT1lGOQj6pdffhnIyormf1sqSyWlTA59Rv1UiVRF5oMHD27x3ymbRRG2nq/5Bfnss8/KngeVhcQuuOACAM4555yOfJSaGjlyJADjxo0D4PzzzwdgxowZANx6660AzJs3D8iiz+uuuw5IOwIXzSFpxabGvEtLMUO2VqO1DRS0WvHJJ58EsvmZetXSyk21XVf17bXPPvsA2XdA3y/1KbXgSNzMLGFJROKiovS33XYbkI3bvvPOOwDssMMOQPbbderUqQBsv/32QGUp2muvvRaAgQMHdl6jO4kiZ0XKipJ1W5kGl156KZCtQNQxyEfVuq1VfMrOKc3S0Uy+MjlUq0bjyCls+aYNEfS5tdmDMjY0b6KrDNVWaSTLL788kNX50NVYnuaiWqPv17Bhw4BsHUa92mabbZr/rvNdkbPK8WpDcdVZKt3SrfT5reWZv/rqq0D2/RwwYEBV2v5nHImbmSUsqUhckZ9qPVx55ZVlj2uMXNutaZw4v6pKY3cLyy+vZ4qYRZ9NNVTykbZ+ahxUEYLmCTS2rporLc0PaKXfFltsAVSOw6dAK+m0sk51c1TJ77XXXgOyWiqNTOPAizoerCqid955Z7Wa1KnWWWed5r/r+6BMtXvvvRfIrirz9P3QnIhyzjV3ois4fe+eeuopwJG4mZktRFKRuCijQPUsNKuuSPHLL78se75qpWhs/OabbwbSygvP01i1ts5S3rgoIlBkrYqPmk1fGK0ALaXsIM1FqA2q01LPlAs9fPjwFh/XxrmLQwTeUVrxrDklfb9SNHfuXCC7Qttrr70A2HTTTQHYeOONgWwtSd6cOXMA2GqrrYBsVXktORI3M0tYkpG4VpwpF3ro0KEAnHTSSS0+X2PfjRCBi7JOVBc7P/YtqtKnrBUdgzzNpmt8O1+LBbLoXuPqyuyoxbjfotLqXkVU3333XdnjGtfVOWQLp92PNH6cn5+pV6W1U4488kgA1l57bQD2339/oP0bYGtlr/6dI3EzM2uXJCPxvIXtrqE88kMPPRTI8shTU7rvqManFSmrfoUibuXwKmNHdWcWlieuaEVXOcpeKX2O6kWkcEVz2GGHAVkErrFOrfJVrvTo0aOBRc/USMnYsWOB9q+w1Dl1xx13AFmWUn7P2nrVo0eP5r/fdNNNVX1tzTWNGjWqqq/bFo7EzcwS1hCReH48WONUPXv2BLIIQqsWlUeuHa/rPV9cWTeqGgfZZ1VU/MADDwBZJK7aJ4rENY79yCOPlD0vnyeunynvNwpZXXCN33br1g3Ixv5feuklIIvEP/nkkxq3sPaUmaPvQVtXWKpW0c477wxkmT6aXyndD3dxpT5F8rWIOpMjcTOzhDVEJJ6nuhca11VNFUW0WpWnSOS4446rdRPbRbWPlZ8NWSZOftVqPqLWGLDkbzequ+++G8jqvehc0ErUtdZaq5iGFUgZR8rzVp2PfBQpisB17HT+ae2BdgYyWHfddctuv/HGG0C21wFkO5RVmyNxM7OENWQkrnFgRavKPFBEoYhc2R71Honnx7WhPGvEKm244YZlt/N7jnbp0qXs9vTp0zu9TUXT/Ilqqh977LFAFlHrceX/K6tLEbgeVxVRVUO0jOYNdLWj7y44Ejczsxa4EzczS1hobYFMFXX6G+RTDLVgRUuptRmuNrvVRq+ixzu4eWpY+FOateuYaNhHZVMhmfKf7TkmUMVzRWVytaz666+/BrIiaEqxVNF/LYYaMWJEtZrwZzrtXGkLbTV39dVXAzBlypSW3zhXRE2TdZ00jFLoMamWU045BciOrQpqQflivTZq0zFxJG5mlrCGiMRVmra17cH69u0LZGUn86VqFYnrt+ciaohIosoKi8RFm2RfccUVQOsLXPR/f+KJJ1a7CS2pi3NFKYS6YtWCMXn88ceBbOJTi+c6SV0ck47ShjUqSVCaNLEICRSOxM3MGl1DROJKgdJGyhMmTACygu35Yk+isXMl5HewpGZDRBJVVngkXqd8rlTyMankSNzMrNE1RCSep7E8jXPmS6jKwIEDgWzDgA5yJFHJkXjLfK5U8jGp5EjczKzRNWQkXhBHEpUcibfM50olH5NKjsTNzBqdO3Ezs4S5EzczS1gtxsTNzKyTOBI3M0uYO3Ezs4S5EzczS5g7cTOzhLkTNzNLmDtxM7OEuRM3M0uYO3Ezs4S5EzczS5g7cTOzhLkTNzNLmDtxM7OEuRM3M0uYO3Ezs4S5EzczS5g7cTOzhLkTNzNLmDtxM7OEuRM3M0uYO3Ezs4S5EzczS9j/A9UGjVp1CoZpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    for index in range(5):\n",
    "        plt.subplot(1, 5, index + 1)\n",
    "        image = images[index, ..., 0]\n",
    "        label = labels[index].numpy()\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "    break # just showing part of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aecf77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
