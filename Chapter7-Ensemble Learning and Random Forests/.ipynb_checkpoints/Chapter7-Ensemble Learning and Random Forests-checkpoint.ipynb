{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bb4d99",
   "metadata": {},
   "source": [
    "# 投票分类器\n",
    "## 硬分类器\n",
    "- 大多数投票分类器称为硬分类器。\n",
    "- 即是每个都是弱学习器，通过集成依然可以实现一个强学习器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9b34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建并训练一个投票分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f6cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf=LogisticRegression()\n",
    "rnd_clf=RandomForestClassifier()\n",
    "svm_clf=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcab6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf=VotingClassifier(estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],voting='hard')# 硬分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0defe5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4ff993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b52542ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "# 让我们来看一下精确度\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045829f",
   "metadata": {},
   "source": [
    "## 软分类器\n",
    "如果所有分类器都能估算类别的概率(即有predict_proba())，可以将平均概率最高的类别作为预测。这被称为**软投票法**。通常软投票法变现更优。因为它给予那些高度自信的投票更高的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8a2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf=LogisticRegression()\n",
    "rnd_clf=RandomForestClassifier()\n",
    "svm_clf=SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa4e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf=VotingClassifier(estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611f3127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()),\n",
       "                             ('svc', SVC(probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a052d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "for clf in [log_clf,rnd_clf,svm_clf,voting_clf]:\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea16e9",
   "metadata": {},
   "source": [
    "# bagging和pasting\n",
    "- 另一种方法是，每个预测器使用的算法相同，但是在不同的训练集随机子集上进行训练。\n",
    "- 采样时如果将样本放回，这种方法叫做bagging。\n",
    "- 采样时样本不放回，这种方法叫pasting。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213ee98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd5f460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf=BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,n_jobs=-1)\n",
    "# 500个弱分类区，每次采样100个实例\n",
    "# 如果想采用pasting,boostrap=False\n",
    "bag_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83228c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245ff56",
   "metadata": {},
   "source": [
    "集成的方法相比于单个预测器偏差相近，但是方差更小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b112755",
   "metadata": {},
   "source": [
    "## 包外估计\n",
    "由于自助抽样，平均只对63%的训练实例进行采样，而剩余37%的训练实例我们成为包外(oob)实例。由于预测器在训练过程中从未看到oob实例，因此可以在这些实例上进行评估而不需要单独的验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13f3396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf=BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=500,bootstrap=True,n_jobs=-1,oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8c22b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们先看看包外估计的分数\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61ca331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e329152",
   "metadata": {},
   "source": [
    "可以看出二者足够接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7d5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35      , 0.65      ],\n",
       "       [0.38095238, 0.61904762],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10240964, 0.89759036],\n",
       "       [0.37724551, 0.62275449],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.78977273, 0.21022727],\n",
       "       [0.0199005 , 0.9800995 ],\n",
       "       [0.74860335, 0.25139665],\n",
       "       [0.8972973 , 0.1027027 ],\n",
       "       [0.96385542, 0.03614458],\n",
       "       [0.07729469, 0.92270531],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9804878 , 0.0195122 ],\n",
       "       [0.94974874, 0.05025126],\n",
       "       [0.98843931, 0.01156069],\n",
       "       [0.00653595, 0.99346405],\n",
       "       [0.36111111, 0.63888889],\n",
       "       [0.93513514, 0.06486486],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96256684, 0.03743316],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62087912, 0.37912088],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15934066, 0.84065934],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.        , 1.        ],\n",
       "       [0.39896373, 0.60103627],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.25414365, 0.74585635],\n",
       "       [0.33142857, 0.66857143],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01015228, 0.98984772],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [0.9009901 , 0.0990099 ],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04736842, 0.95263158],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [0.77777778, 0.22222222],\n",
       "       [0.375     , 0.625     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66850829, 0.33149171],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84126984, 0.15873016],\n",
       "       [1.        , 0.        ],\n",
       "       [0.609375  , 0.390625  ],\n",
       "       [0.17058824, 0.82941176],\n",
       "       [0.64705882, 0.35294118],\n",
       "       [0.92346939, 0.07653061],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15      , 0.85      ],\n",
       "       [0.91954023, 0.08045977],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0441989 , 0.9558011 ],\n",
       "       [0.04918033, 0.95081967],\n",
       "       [0.28176796, 0.71823204],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00492611, 0.99507389],\n",
       "       [0.82022472, 0.17977528],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25742574, 0.74257426],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95073892, 0.04926108],\n",
       "       [0.81609195, 0.18390805],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.16483516, 0.83516484],\n",
       "       [0.63855422, 0.36144578],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04891304, 0.95108696],\n",
       "       [0.5177665 , 0.4822335 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29050279, 0.70949721],\n",
       "       [0.51282051, 0.48717949],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [0.32317073, 0.67682927],\n",
       "       [0.93513514, 0.06486486],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.78527607, 0.21472393],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00515464, 0.99484536],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95135135, 0.04864865],\n",
       "       [0.98802395, 0.01197605],\n",
       "       [0.02857143, 0.97142857],\n",
       "       [0.17159763, 0.82840237],\n",
       "       [0.96610169, 0.03389831],\n",
       "       [0.25806452, 0.74193548],\n",
       "       [0.98979592, 0.01020408],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00947867, 0.99052133],\n",
       "       [0.77717391, 0.22282609],\n",
       "       [0.42777778, 0.57222222],\n",
       "       [0.39810427, 0.60189573],\n",
       "       [0.88823529, 0.11176471],\n",
       "       [0.92753623, 0.07246377],\n",
       "       [0.07853403, 0.92146597],\n",
       "       [0.80446927, 0.19553073],\n",
       "       [0.00502513, 0.99497487],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03664921, 0.96335079],\n",
       "       [0.96236559, 0.03763441],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01796407, 0.98203593],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95      , 0.05      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40659341, 0.59340659],\n",
       "       [0.22651934, 0.77348066],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32163743, 0.67836257],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9804878 , 0.0195122 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.005     , 0.995     ],\n",
       "       [0.64942529, 0.35057471],\n",
       "       [0.93048128, 0.06951872],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.99019608, 0.00980392],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.09944751, 0.90055249],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05988024, 0.94011976],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03278689, 0.96721311],\n",
       "       [1.        , 0.        ],\n",
       "       [0.90857143, 0.09142857],\n",
       "       [0.74842767, 0.25157233],\n",
       "       [0.59898477, 0.40101523],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [0.17277487, 0.82722513],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95833333, 0.04166667],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44600939, 0.55399061],\n",
       "       [0.86857143, 0.13142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97076023, 0.02923977],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2606383 , 0.7393617 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98469388, 0.01530612],\n",
       "       [0.87647059, 0.12352941],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08064516, 0.91935484],\n",
       "       [0.99401198, 0.00598802],\n",
       "       [0.02259887, 0.97740113],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05586592, 0.94413408],\n",
       "       [0.98333333, 0.01666667],\n",
       "       [0.81280788, 0.18719212],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85227273, 0.14772727],\n",
       "       [0.98850575, 0.01149425],\n",
       "       [0.21311475, 0.78688525],\n",
       "       [0.14754098, 0.85245902],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25668449, 0.74331551],\n",
       "       [0.96791444, 0.03208556],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.        , 1.        ],\n",
       "       [0.50738916, 0.49261084],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08139535, 0.91860465],\n",
       "       [0.11170213, 0.88829787],\n",
       "       [1.        , 0.        ],\n",
       "       [0.015625  , 0.984375  ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.40217391, 0.59782609],\n",
       "       [0.11688312, 0.88311688],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.64285714, 0.35714286],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.64804469, 0.35195531],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23563218, 0.76436782],\n",
       "       [0.79768786, 0.20231214],\n",
       "       [0.09433962, 0.90566038],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77894737, 0.22105263],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13744076, 0.86255924],\n",
       "       [0.02222222, 0.97777778],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [0.88944724, 0.11055276],\n",
       "       [0.23295455, 0.76704545],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.60555556, 0.39444444],\n",
       "       [0.06315789, 0.93684211],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.80662983, 0.19337017],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.93846154, 0.06153846],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27956989, 0.72043011],\n",
       "       [0.98173516, 0.01826484],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [0.82901554, 0.17098446],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74342105, 0.25657895],\n",
       "       [0.93922652, 0.06077348],\n",
       "       [1.        , 0.        ],\n",
       "       [0.69072165, 0.30927835],\n",
       "       [0.52197802, 0.47802198],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91      , 0.09      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88359788, 0.11640212],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7032967 , 0.2967033 ],\n",
       "       [0.12903226, 0.87096774],\n",
       "       [0.46195652, 0.53804348],\n",
       "       [0.26011561, 0.73988439],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90909091, 0.09090909],\n",
       "       [0.83060109, 0.16939891],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02702703, 0.97297297],\n",
       "       [0.95402299, 0.04597701],\n",
       "       [0.96470588, 0.03529412],\n",
       "       [1.        , 0.        ],\n",
       "       [0.45945946, 0.54054054],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.01515152, 0.98484848],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95428571, 0.04571429],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0483871 , 0.9516129 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99415205, 0.00584795],\n",
       "       [0.01052632, 0.98947368],\n",
       "       [1.        , 0.        ],\n",
       "       [0.15104167, 0.84895833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42622951, 0.57377049],\n",
       "       [0.08      , 0.92      ],\n",
       "       [0.21978022, 0.78021978],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.185     , 0.815     ],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95108696, 0.04891304],\n",
       "       [0.38953488, 0.61046512],\n",
       "       [0.98429319, 0.01570681],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04371585, 0.95628415],\n",
       "       [0.97916667, 0.02083333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05027933, 0.94972067],\n",
       "       [0.59358289, 0.40641711]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9e583",
   "metadata": {},
   "source": [
    "# 随机补丁和随机子空间\n",
    "- BaggingClassifier()也支持对特征进行采样。由超参数max_features和bootstrap_features控制。每个预测器将用输入特征的随机子集进行训练。\n",
    "- 对训练实例和特征都进行抽样，这称为**随机补丁方法**。\n",
    "- 而保留所有训练实例，但对训练特征进行抽样称为**随机子空间法**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6147b01",
   "metadata": {},
   "source": [
    "# 随机森林\n",
    "除了先构建BaggingClassifier然后将其传输到DecisionTreeClassifier，还有一种方法是使用RandomForestClassifier类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caaf5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b19d33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf=RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)# 每棵树限制最多16个叶节点\n",
    "rnd_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e84c30d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2720f3a",
   "metadata": {},
   "source": [
    "随机森林分裂结点时，不再是搜索最好的特征，而是在一个随机生成的特征子集上搜索最好的特征，这导致决策树具有很高的多样性，用更高的偏差换取更低的方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7064ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的BaggingClassifier与RandomForestClassifier相同\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(\n",
    "    splitter='random', max_leaf_nodes=16), n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "# boostrap=False是采用pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c1b32",
   "metadata": {},
   "source": [
    "## 极端随机树\n",
    "见书。\n",
    "## 特征重要性\n",
    "随机森林的另一个好特性是它们使测量每个特征的相对重要性变得容易。sklearn通过查看使用该特征的树节点平均(在森林中的所有树上)减少不纯度的程度来衡量该特征的重要性。sklearn会在训练后为每个特征自动计算该分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7314cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用鸢尾花数据来测试\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "376fae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf=RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "rnd_clf.fit(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bf24da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09317747554141863\n",
      "sepal width (cm) 0.02326474213136857\n",
      "petal length (cm) 0.42953256347482033\n",
      "petal width (cm) 0.4540252188523924\n"
     ]
    }
   ],
   "source": [
    "for name,score in zip(iris.feature_names,rnd_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b53841",
   "metadata": {},
   "source": [
    "# 提升法\n",
    "## AdaBoost\n",
    "理论部分见书。sklearn使用的是AdaBoost的一个多分类版本，叫做SAMME。当只有两类时,SAMME等同于AdaBoost。此外如果预测器可以预测概率,sklearn会使用一种SAMME的变体，SAMME.R。它依赖的是类概率，通常表现更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8874a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42d2808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn的AdaBoostClassifier训练一个AdaBoost分类器，它基于200个单层决策树。顾名思义，max_depth=1的决策树。\n",
    "ada_clf=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=200,algorithm='SAMME.R',learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da3ef11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e01779d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=ada_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e076882",
   "metadata": {},
   "source": [
    "## 梯度提升\n",
    "它不像AdaBoost一样在每个迭代中调整实例权重，而是让新的预测器针对前一个预测器的迭代残差进行拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0f46408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们看一个简单的回归实例\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.rand(100, 1) - 0.5\n",
    "y_reg = 3*X_reg[:,0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe653ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg1=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_reg,y_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91d7626f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reg2=y_reg-tree_reg1.predict(X_reg)# 拟合残差\n",
    "tree_reg2=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_reg,y_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e1a15cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reg3=y_reg2-tree_reg2.predict(X_reg)\n",
    "tree_reg3=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_reg,y_reg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "700459f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=0.0\n",
    "X_new = np.array([[0.8]])\n",
    "for tree in (tree_reg1,tree_reg2,tree_reg3):\n",
    "    y_pred+=tree.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d18e6",
   "metadata": {},
   "source": [
    "训练GBDR有GradientBoostingRegressor类。\n",
    "- 超参数learning_rate对每棵树的贡献进行缩放。如果将其设置为低值，则需要更多的树来拟合训练集，但是预测泛化的效果通常更好。这是一种被称为收缩的正则化技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32d03235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingRegressor类\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt=GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=0.1)\n",
    "gbrt.fit(X_reg,y_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e68a5a",
   "metadata": {},
   "source": [
    "要找到树的最佳数量，可以用提前停止法。简单的实现方法就是使用staged_predict()方法。在它训练的每个阶段(训练一棵树、两棵树...)。都对集成的预测返回一个迭代器，然后测量每个训练阶段的验证误差，从而找到最优数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a124cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5adcf608",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X_reg,y_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55aa8e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt=GradientBoostingRegressor(max_depth=2,n_estimators=120)\n",
    "gbrt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd5483fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=[mean_squared_error(y_val,y_pred) for y_pred in gbrt.staged_predict(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d72f6a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04853645651402395,\n",
       " 0.040640268941000264,\n",
       " 0.034251627375732265,\n",
       " 0.030860253423403018,\n",
       " 0.026963941731481037,\n",
       " 0.023139718761101085,\n",
       " 0.02118892032500619,\n",
       " 0.018172837139291655,\n",
       " 0.01658469511059632,\n",
       " 0.014568427284013214,\n",
       " 0.012919592933029175,\n",
       " 0.012030899023042343,\n",
       " 0.011188765359382204,\n",
       " 0.010390290551727637,\n",
       " 0.009306394142702906,\n",
       " 0.00830459415632203,\n",
       " 0.007626346083680286,\n",
       " 0.007373861189333121,\n",
       " 0.007136295600513196,\n",
       " 0.006741763823569835,\n",
       " 0.006102481400713888,\n",
       " 0.005690836404214948,\n",
       " 0.005291986768518243,\n",
       " 0.005172467397888522,\n",
       " 0.004935572194959045,\n",
       " 0.004746297700031881,\n",
       " 0.0044323394223554375,\n",
       " 0.004244265295864291,\n",
       " 0.0040111971146731635,\n",
       " 0.003954394710620797,\n",
       " 0.003879733231164774,\n",
       " 0.0037918509916983934,\n",
       " 0.0037034180984788813,\n",
       " 0.0035410961960233396,\n",
       " 0.003497527610255329,\n",
       " 0.0033465574270547544,\n",
       " 0.0032133362689326134,\n",
       " 0.0031668820700545526,\n",
       " 0.0031474880551944522,\n",
       " 0.00313290434626548,\n",
       " 0.003060384155496198,\n",
       " 0.0030517634820494016,\n",
       " 0.003041398686667091,\n",
       " 0.0029695000264205783,\n",
       " 0.0029454897235765094,\n",
       " 0.0029243959847866314,\n",
       " 0.002899562625481585,\n",
       " 0.0028793532295200972,\n",
       " 0.0028306254869693436,\n",
       " 0.0028375495739460005,\n",
       " 0.0028010096903141263,\n",
       " 0.0027964503749681696,\n",
       " 0.00279168230763824,\n",
       " 0.0027768248128625874,\n",
       " 0.0028161869789869766,\n",
       " 0.002791135865934465,\n",
       " 0.0027966587933267017,\n",
       " 0.0027937454480891155,\n",
       " 0.0027689568526369983,\n",
       " 0.002756040890779319,\n",
       " 0.0027626353600367437,\n",
       " 0.0027731416226252402,\n",
       " 0.0027787051975619844,\n",
       " 0.002750279033345716,\n",
       " 0.002776730794241967,\n",
       " 0.0027824729359416356,\n",
       " 0.002770872196710712,\n",
       " 0.0027692494627763525,\n",
       " 0.0027511651083637644,\n",
       " 0.00276928636665117,\n",
       " 0.002757781607202615,\n",
       " 0.0027633835376754444,\n",
       " 0.002776936570773492,\n",
       " 0.0028023056091986544,\n",
       " 0.0028029862868812904,\n",
       " 0.0027993447167391832,\n",
       " 0.0027993458745314464,\n",
       " 0.0027979458401205474,\n",
       " 0.0027942328213067125,\n",
       " 0.0027758551104791057,\n",
       " 0.0027814952546637624,\n",
       " 0.0027718998215120277,\n",
       " 0.0027666932513486964,\n",
       " 0.002767296974351304,\n",
       " 0.0027426595994411397,\n",
       " 0.002767599159143672,\n",
       " 0.0027742353833165793,\n",
       " 0.0027788659010792817,\n",
       " 0.0027698183412890494,\n",
       " 0.0027747314258677624,\n",
       " 0.002775038411135105,\n",
       " 0.0027629528215528548,\n",
       " 0.002763490927816061,\n",
       " 0.0027465046182896134,\n",
       " 0.002779441316983275,\n",
       " 0.00277321149846918,\n",
       " 0.0027969415127717546,\n",
       " 0.0027909082981709753,\n",
       " 0.002782927206760999,\n",
       " 0.0027874098179574477,\n",
       " 0.0027874769479952722,\n",
       " 0.002787668865147023,\n",
       " 0.0027882108396286337,\n",
       " 0.0027850571783408,\n",
       " 0.00280124312223706,\n",
       " 0.0028015740981667775,\n",
       " 0.002795818597191323,\n",
       " 0.002796734572132251,\n",
       " 0.0028093432580732513,\n",
       " 0.0028237663255249405,\n",
       " 0.002832998189236506,\n",
       " 0.0028330350783265256,\n",
       " 0.002827395577525056,\n",
       " 0.002827791414611322,\n",
       " 0.002831638994373167,\n",
       " 0.002831859767748081,\n",
       " 0.0028457168660203282,\n",
       " 0.0028459963899941736,\n",
       " 0.0028564645562287427,\n",
       " 0.002856492512346977]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "746e6a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=85)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n_estimators=np.argmin(errors)+1 # 找到最优的树的数量\n",
    "gbrt_best=GradientBoostingRegressor(max_depth=2,n_estimators=best_n_estimators)\n",
    "gbrt_best.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b3085",
   "metadata": {},
   "source": [
    "实际上要实现提前停止法，不一定需要训练大量的树，还可以提前停止训练。设置warm_start=True，当fit()方法被调用时，sklearn会保留现有的树，从而允许增量训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fd3d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt=GradientBoostingRegressor(max_depth=2,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b114dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_error=np.float('inf')\n",
    "error_going_up=0\n",
    "for n_estimators in range(1,121):\n",
    "    gbrt.n_estimators=n_estimators\n",
    "    gbrt.fit(X_train,y_train)\n",
    "    y_pred=gbrt.predict(X_val)\n",
    "    val_error=mean_squared_error(y_val,y_pred)\n",
    "    if val_error<min_val_error:\n",
    "        min_val_error=val_error\n",
    "        error_going_up=0\n",
    "    else:\n",
    "        error_going_up+=1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d94c5",
   "metadata": {},
   "source": [
    "GradientBoostingRegressor还支持超参数subsample，指定用于训练每棵树的实例的比例。这也是用更高的偏差换取了更低的方差，同时在相当大程度上加速了训练过程。这种技术被称为随机梯度提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ba53e",
   "metadata": {},
   "source": [
    "有机会学习xgboost。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d503d3d",
   "metadata": {},
   "source": [
    "# 堆叠法\n",
    "又称层叠泛化法。它基于这样一个简单的想法：与其使用一些简单的函数来聚合集成中所有预测器的预测，不如训练一个模型来执行这个聚合。详细见书。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b56b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
